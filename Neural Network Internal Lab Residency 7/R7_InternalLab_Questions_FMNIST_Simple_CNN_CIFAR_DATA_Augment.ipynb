{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyfMmMnPJjvn"
   },
   "source": [
    "## Train a simple convnet on the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjcGOJhcJjvp"
   },
   "source": [
    "In this, we will see how to deal with image data and train a convnet for image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR0Pl2XjJjvq"
   },
   "source": [
    "### Load the  `fashion_mnist`  dataset\n",
    "\n",
    "** Use keras.datasets to load the dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr75v_UYJjvs"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTI42-0qJjvw"
   },
   "source": [
    "### Find no.of samples are there in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2sf67VoJjvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zewyDcBlJjv1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (10000, 28, 28) y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 3 0 2 7 2 5 5]\n"
     ]
    }
   ],
   "source": [
    "#Original labels (0:10) from the train-dataset\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7]\n"
     ]
    }
   ],
   "source": [
    "#Original labels (0:10) from the test-dataset\n",
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WytT2eRnJjv4"
   },
   "source": [
    "### Find dimensions of an image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XycQGBSGJjv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape:\", x_train [0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jtdZ7RqJjv8"
   },
   "source": [
    "### Convert train and test labels to one hot vectors\n",
    "\n",
    "** check `keras.utils.to_categorical()` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAD3q5I6Jjv9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert train and test labels to one hot vectors\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes=10, dtype='float32')\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgHSCXy3JjwA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xO5BRBzBJjwD"
   },
   "source": [
    "### Normalize both the train and test image data from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fUQpMHxJjwE"
   },
   "outputs": [],
   "source": [
    "#Normalize both the train and test image data from 0-255 to 0-1\n",
    "X_train, X_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Okwo_SB5JjwI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary layers from keras to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "da5-DwgrJjwM"
   },
   "source": [
    "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPGVQ-JJJjwN"
   },
   "outputs": [],
   "source": [
    "#Initialize model, reshape & normalize data\n",
    "model = tf.keras.models.Sequential()\n",
    "#Reshape data from 2D (28,28) to 3D (28, 28, 1)\n",
    "#model.add(tf.keras.layers.Reshape((28,28,1),input_shape=(28,28,)))\n",
    "#normalize data\n",
    "#model.add(tf.keras.layers.BatchNormalization(input_shape=(28,28,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C18AoS7eJjwU"
   },
   "source": [
    "### Build a model \n",
    "\n",
    "** with 2 Conv layers having `32 3x3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping with patience=5\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DORCLgSwJjwV"
   },
   "outputs": [],
   "source": [
    "#Add first convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
    "                                 kernel_size=(3,3), #Size of the filter\n",
    "                                 activation='relu', input_shape=(28,28,1)))\n",
    "#Add second convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.output\n",
    "#Flatten the output\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "#Dense layer\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "#Output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_52 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               2359424   \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,370,282\n",
      "Trainable params: 2,370,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 23s 377us/sample - loss: 0.3828 - acc: 0.8620 - val_loss: 0.3025 - val_acc: 0.8916\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 22s 362us/sample - loss: 0.2391 - acc: 0.9128 - val_loss: 0.2530 - val_acc: 0.9084\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 22s 362us/sample - loss: 0.1723 - acc: 0.9352 - val_loss: 0.2545 - val_acc: 0.9093\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 22s 363us/sample - loss: 0.1193 - acc: 0.9559 - val_loss: 0.2862 - val_acc: 0.9118\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.0791 - acc: 0.9706 - val_loss: 0.3135 - val_acc: 0.9103\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.0535 - acc: 0.9800 - val_loss: 0.3830 - val_acc: 0.9071\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 22s 365us/sample - loss: 0.0364 - acc: 0.9868 - val_loss: 0.4607 - val_acc: 0.9097\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.0282 - acc: 0.9898 - val_loss: 0.4730 - val_acc: 0.9107\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.0251 - acc: 0.9914 - val_loss: 0.4853 - val_acc: 0.9140\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.0198 - acc: 0.9931 - val_loss: 0.5599 - val_acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bb25e04a8>"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(X_train,Y_train,          \n",
    "          validation_data=(X_test,Y_test),\n",
    "          epochs=10,\n",
    "          batch_size=32, \n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju69vKdIJjwX"
   },
   "source": [
    "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model, reshape & normalize data\n",
    "model1 = tf.keras.models.Sequential()\n",
    "#Reshape data from 2D (28,28) to 3D (28, 28, 1)\n",
    "#model1.add(tf.keras.layers.Reshape((28,28,1),input_shape=(28,28,)))\n",
    "#normalize data\n",
    "#model1.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add first convolutional layer\n",
    "model1.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
    "                                 kernel_size=(3,3), #Size of the filter\n",
    "                                 activation='relu', input_shape=(28,28,1)))\n",
    "#Add second convolutional layer\n",
    "model1.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2hAP94vJjwY"
   },
   "outputs": [],
   "source": [
    "#Add MaxPooling layer\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_6/MaxPool:0' shape=(?, 12, 12, 32) dtype=float32>"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the output\n",
    "model1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Dense layer\n",
    "model1.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#Add another dropout layer\n",
    "model1.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "#Output layer\n",
    "model1.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model1.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_57 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 21s 344us/sample - loss: 0.4070 - acc: 0.8535 - val_loss: 0.2915 - val_acc: 0.8931\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 20s 328us/sample - loss: 0.2636 - acc: 0.9043 - val_loss: 0.2630 - val_acc: 0.9043\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 0.2191 - acc: 0.9189 - val_loss: 0.2390 - val_acc: 0.9129\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 0.1835 - acc: 0.9316 - val_loss: 0.2319 - val_acc: 0.9174\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 20s 331us/sample - loss: 0.1561 - acc: 0.9414 - val_loss: 0.2254 - val_acc: 0.9217\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 20s 326us/sample - loss: 0.1341 - acc: 0.9493 - val_loss: 0.2394 - val_acc: 0.9232\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 20s 328us/sample - loss: 0.1144 - acc: 0.9565 - val_loss: 0.2467 - val_acc: 0.9237\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 0.0950 - acc: 0.9639 - val_loss: 0.2654 - val_acc: 0.9224\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 20s 333us/sample - loss: 0.0829 - acc: 0.9682 - val_loss: 0.2828 - val_acc: 0.9236\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 20s 326us/sample - loss: 0.0728 - acc: 0.9725 - val_loss: 0.2938 - val_acc: 0.9234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bbcffe358>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model1.fit(X_train,Y_train,          \n",
    "          validation_data=(X_test,Y_test),\n",
    "          epochs=10,\n",
    "          batch_size=32, \n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGTA3bfEJjwa"
   },
   "source": [
    "### Now, to the above model, lets add Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6gX8n5SJjwb"
   },
   "source": [
    "### Import the ImageDataGenrator from keras and fit the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "#X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cbz4uHBuJjwc"
   },
   "outputs": [],
   "source": [
    "transform_fn = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                               width_shift_range=0.2,\n",
    "                                                               height_shift_range=0.2,\n",
    "                                                               rotation_range=30,\n",
    "                                                               shear_range=0.2,\n",
    "                                                               zoom_range=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl-8dOo7Jjwf"
   },
   "source": [
    "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpI1_McYJjwg",
    "outputId": "6722631e-c925-448c-c780-93a3100249bc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "gen = transform_fn.flow(X_train[0:1], batch_size=1)\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
    "    plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmPl5yE8Jjwm"
   },
   "source": [
    "### Run the above model using fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.7003 - acc: 0.7378 - val_loss: 0.5093 - val_acc: 0.8015\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.6561 - acc: 0.7563 - val_loss: 0.5235 - val_acc: 0.8080\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.6220 - acc: 0.7679 - val_loss: 0.4580 - val_acc: 0.8307\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.6046 - acc: 0.7747 - val_loss: 0.6079 - val_acc: 0.7901: 0s - loss: 0.6038 -\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.5836 - acc: 0.7833 - val_loss: 0.5203 - val_acc: 0.8158\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.5679 - acc: 0.7894 - val_loss: 0.4737 - val_acc: 0.8351\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.5606 - acc: 0.7934 - val_loss: 0.5514 - val_acc: 0.8119\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.5499 - acc: 0.7972 - val_loss: 0.5541 - val_acc: 0.8101\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.5363 - acc: 0.8040 - val_loss: 0.5440 - val_acc: 0.8177\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.5344 - acc: 0.8003 - val_loss: 0.5712 - val_acc: 0.8129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bbd3ab128>"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(transform_fn.flow(X_train, Y_train, batch_size = 32),\n",
    " validation_data = (X_test, Y_test), steps_per_epoch = len(X_train) // 32,\n",
    " epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwQQW5iOJjwq"
   },
   "source": [
    "###  Report the final train and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1SrtBEPJjwq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBwVWNQC2qZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KXqmUDW2rM1"
   },
   "source": [
    "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mja6OgQ3L18"
   },
   "source": [
    "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HzVTPUM3WZJ"
   },
   "source": [
    "### **Import neessary libraries for data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPM558TX4KMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6hicLwP4SqY"
   },
   "source": [
    "### **Load CIFAR10 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ1WzrXd4WNk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9Pht1ggHuiT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3n28ccU6Hp6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN3vYYhK4W0u"
   },
   "source": [
    "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJbekTKi4cmM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-SLtUhC4dK2"
   },
   "source": [
    "### **Prepare/fit the generator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSw8Bv2_4hb0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYyF-P8O4jQ8"
   },
   "source": [
    "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXug4z234mwQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
